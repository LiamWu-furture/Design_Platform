好的，我们来详细说明使用 LangChain 框架时，向量数据库的**保存（持久化）和使用（检索）**过程。我们将继续以最常见的开源向量数据库 Chroma 为例，因为它支持本地运行和数据的持久化（即保存到磁盘）。💾 步骤一：向量数据库的保存（持久化 - Persistence）持久化是将向量和相关的原始文本及元数据写入到磁盘的过程，这样即使程序关闭，数据也不会丢失。1. 初始化和存储在 LangChain 中，当您第一次使用 Chroma.from_documents 方法创建向量数据库时，您需要指定一个 persist_directory 参数。Pythonfrom langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# 假设 chunks 是经过切块的 Document 列表
# 假设 embedding_model 是已配置的嵌入模型

PERSIST_DIR = "./my_professional_vector_db" # 指定数据保存的本地路径

# 核心操作：创建并存储
# 这一步会执行：切块 -> 向量化 -> 存储到内存 -> 写入磁盘
vector_db = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory=PERSIST_DIR  # 关键参数：指定保存路径
)

# 显式调用 persist() 确保数据立即写入磁盘 (虽然 from_documents 可能会自动执行)
vector_db.persist()
print(f"向量数据库已成功保存到磁盘路径: {PERSIST_DIR}")
2. 保存的内容当数据被持久化后，在 ./my_professional_vector_db 文件夹中，Chroma 会生成一系列文件，这些文件共同保存了以下信息：向量本身： 由嵌入模型生成的高维数字数组。原始文本块： 向量对应的原始文本片段。元数据： 包括文章来源、标题、页码等信息。索引结构： Chroma 用于快速查找相似向量的底层索引文件（如 HNSW 结构）。🔎 步骤二：向量数据库的使用（加载与检索）一旦数据库被保存，在后续的任何程序运行时，您无需重新进行切块和向量化，只需加载它即可投入使用。1. 数据库的加载（Loading）使用 Chroma 类的 from_existing_persist_dir（或 Chroma 构造函数）方法加载已保存的数据。Python# 不需要再次传入 documents，因为数据已在磁盘上
PERSIST_DIR = "./my_professional_vector_db"
# 重新加载嵌入模型，因为 Chroma 需要它来将查询转换为向量
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-en-v1.5"
)

# 核心操作：加载已保存的向量数据库
vector_db = Chroma(
    persist_directory=PERSIST_DIR,
    embedding_function=embedding_model # 必须传入嵌入模型，用于后续的查询向量化
)

print(f"向量数据库已成功从 {PERSIST_DIR} 路径加载。")
2. 数据库的使用（检索 - Retrieval）加载完成后，下一步是将其转换为 LangChain 的核心组件之一：检索器（Retriever）。A. 创建检索器检索器负责处理用户查询并返回最相关的文档块。Python# 创建检索器实例
retriever = vector_db.as_retriever(
    search_type="similarity", # 使用最常见的余弦相似度搜索
    search_kwargs={"k": 5}    # 检索最相似的 5 个文档块
)

print("检索器已准备就绪。")
B. 检索过程的内部机制当用户输入一个问题时，检索器会执行以下操作：查询向量化： 用户的文本查询（如“镍合金的蠕变极限”）被传入 embedding_model，生成一个查询向量。相似性搜索： 检索器将查询向量与数据库中所有保存的向量进行比较，计算它们的相似度（如余弦相似度）。返回结果： 检索器返回 $k$ 个相似度最高（距离最近）的向量所对应的原始文本块及其元数据。C. 将检索器应用于 RAG 链最后，这个检索器被集成到 RetrievalQA Chain 中，与 LLM 结合完成预测和回答：Pythonfrom langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI

# 1. 配置 LLM
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash") 

# 2. 构建 RAG Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff", 
    retriever=retriever, # 传入我们加载并创建的检索器
    return_source_documents=True 
)

# 3. 执行查询
query = "镍合金在高温下的微观结构稳定性如何？"
response = qa_chain.invoke({"query": query})

print("\n--- 完整预测与引用 ---")
print(response['result'])
# LLM 结合检索到的专业知识生成了答案
总结： LangChain 通过 Chroma 模块，将复杂的向量数据库操作封装成了简单的 from_documents（用于保存）和 Chroma(...) + .as_retriever()（用于加载和使用）的 API，极大地简化了专业 RAG 系统的构建。