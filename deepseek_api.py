import os
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

import json
import time
from utils import extract_json_from_text

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨DeepSeekçš„base_url
if DEEPSEEK_API_KEY:
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")
else:
    client = None

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å…‰ç”µæ¢æµ‹å™¨è®¾è®¡ä¸“å®¶ï¼Œç²¾é€šåŠå¯¼ä½“ç‰©ç†ã€ææ–™ç§‘å­¦å’Œå…‰ç”µå™¨ä»¶å·¥ç¨‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„å‚æ•°ï¼Œè®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„å å±‚å…‰ç”µæ¢æµ‹å™¨ã€‚

è®¾è®¡æ—¶éœ€è¦è€ƒè™‘ï¼š
1. ææ–™çš„ç¦å¸¦å®½åº¦ä¸ç›®æ ‡æ³¢é•¿çš„åŒ¹é…
2. å„å±‚åšåº¦å¯¹å…‰å¸æ”¶å’Œè½½æµå­æ”¶é›†çš„å½±å“
3. å¼‚è´¨ç»“ç•Œé¢çš„èƒ½å¸¦åŒ¹é…
4. æš—ç”µæµçš„æŠ‘åˆ¶
5. é‡å­æ•ˆç‡çš„ä¼˜åŒ–

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºè®¾è®¡ç»“æœï¼ŒåŒ…å«layersï¼ˆå±‚ç»“æ„ï¼‰ã€performanceï¼ˆæ€§èƒ½å‚æ•°ï¼‰ã€optimization_suggestionsï¼ˆä¼˜åŒ–å»ºè®®ï¼‰å’Œexplanationï¼ˆè®¾è®¡è¯´æ˜ï¼‰ã€‚
"""

def call_deepseek_api(user_prompt, model="deepseek-reasoner"):
    """
    è°ƒç”¨DeepSeek APIè¿›è¡Œæ¢æµ‹å™¨è®¾è®¡ï¼ˆéæµå¼ï¼‰
    
    Args:
        user_prompt: ç”¨æˆ·æç¤ºè¯
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "deepseek-reasoner" (R1)ï¼Œå¯é€‰ "deepseek-chat" (V3)
    """
    if not client:
        return {
            'status': 'error',
            'message': 'APIå¯†é’¥æœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEY'
        }
    
    try:
        messages = [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': user_prompt}
        ]
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            stream=False
        )
        
        content = response.choices[0].message.content
        
        return {
            'status': 'success',
            'content': content
        }
        
    except Exception as e:
        return {
            'status': 'error',
            'message': f'APIè°ƒç”¨å¤±è´¥: {str(e)}'
        }

def generate_design_stream(prompt, model_type='deepseek-reasoner'):
    """
    ç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæµå¼è°ƒç”¨DeepSeek APIå¹¶è¿”å›ç‰¹å®šæ ¼å¼çš„è¿›åº¦æ•°æ®
    """
    if not client:
        yield json.dumps({
            'step': -1,
            'error': 'APIå¯†é’¥æœªé…ç½®',
            'progress': 0
        }) + '\n'
        return

    # ç¡®å®šæ¨¡å‹æ˜¾ç¤ºåç§°
    model_display = "AIæ¨ç†æ¨¡å‹" if model_type == 'deepseek-reasoner' else "AIå¤§æ¨¡å‹"

    yield json.dumps({'step': 3, 'message': f'è°ƒç”¨{model_display} API', 'progress': 30}) + '\n'
    time.sleep(0.3)
    
    yield json.dumps({'step': 3, 'message': f'ğŸ”— è¿æ¥{model_display} API...', 'progress': 35, 'log': True}) + '\n'
    yield json.dumps({'step': 3, 'message': f'ğŸ“¡ å‘é€è®¾è®¡è¯·æ±‚åˆ°{model_display}æ¨¡å‹...', 'progress': 38, 'log': True}) + '\n'
    yield json.dumps({'step': 3, 'message': f'âœ… è¿æ¥æˆåŠŸï¼Œ{model_display}å¼€å§‹æ¨ç†...', 'progress': 40, 'log': True}) + '\n'

    try:
        messages = [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': prompt}
        ]
        
        response = client.chat.completions.create(
            model=model_type,
            messages=messages,
            stream=True
        )
        
        reasoning_content = ""
        content = ""
        reasoning_count = 0
        content_count = 0
        
        for chunk in response:
            # å¤„ç†æ¨ç†è¿‡ç¨‹ï¼ˆä»…R1æ¨¡å‹æœ‰ï¼‰
            if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                reasoning_chunk = chunk.choices[0].delta.reasoning_content
                reasoning_content += reasoning_chunk
                reasoning_count += 1
                
                # æ¯æ”¶åˆ°5ä¸ªæ¨ç†chunkè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if reasoning_count % 5 == 0:
                    current_progress = min(40 + reasoning_count // 5, 60)
                    preview = reasoning_content[-50:].replace('\n', ' ')
                    yield json.dumps({
                        'step': 4,
                        'message': f' {model_display}æ¨ç†ä¸­: ...{preview}',
                        'progress': current_progress,
                        'log': True
                    }) + '\n'
            
            # å¤„ç†æœ€ç»ˆå†…å®¹
            elif chunk.choices[0].delta.content:
                content_chunk = chunk.choices[0].delta.content
                content += content_chunk
                content_count += 1
                
                # æ¯æ”¶åˆ°3ä¸ªå†…å®¹chunkè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if content_count % 3 == 0:
                    current_progress = min(60 + content_count // 3, 70)
                    yield json.dumps({
                        'step': 4,
                        'message': f'âœ¨ {model_display}ç”Ÿæˆæ–¹æ¡ˆä¸­... (å·²ç”Ÿæˆ {len(content)} å­—ç¬¦)',
                        'progress': current_progress,
                        'log': True
                    }) + '\n'
        
        yield json.dumps({
            'step': 4,
            'message': f' æ¨ç†å®Œæˆï¼æ¨ç† {len(reasoning_content)} å­—ç¬¦ï¼Œæ–¹æ¡ˆ {len(content)} å­—ç¬¦',
            'progress': 75,
            'log': True
        }) + '\n'
        
        # è¿”å›æœ€ç»ˆçš„å®Œæ•´å†…å®¹ï¼Œä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„æ¶ˆæ¯ç±»å‹ï¼Œæˆ–è€…è®©è°ƒç”¨è€…è‡ªå·±è§£æ
        # è¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥yieldç»“æœå¯¹è±¡ï¼Œè€Œæ˜¯è®©è°ƒç”¨è€…çŸ¥é“APIè°ƒç”¨å·²å®Œæˆï¼Œå¹¶æä¾›å†…å®¹
        # ä½†ä¸ºäº†ä¿æŒæµçš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç”Ÿæˆå™¨æœ€åè¿”å›ç»“æœ
        
        yield json.dumps({'step': 5, 'message': 'è§£æè®¾è®¡æ–¹æ¡ˆ', 'progress': 80}) + '\n'
        time.sleep(0.5)

        # ä½¿ç”¨ utils ä¸­çš„å‡½æ•°è§£æ
        design_data = extract_json_from_text(content)
        
        if not design_data:
            yield json.dumps({
                'step': -1,
                'error': 'APIè¿”å›çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œæ— æ³•è§£æä¸ºJSON',
                'progress': 0
            }) + '\n'
            return

        # æˆåŠŸè§£æï¼Œå°†æ•°æ®ä¼ å›
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”¨ä¸€ä¸ªç‰¹æ®Šçš„ type æ¥æ ‡è¯†è¿™æ˜¯æœ€ç»ˆæ•°æ®
        yield json.dumps({
            'type': 'result',
            'design_data': design_data,
            'reasoning_content': reasoning_content
        }) + '\n'

    except Exception as e:
        yield json.dumps({
            'step': -1,
            'error': f'APIè°ƒç”¨å¤±è´¥: {str(e)}',
            'progress': 0
        }) + '\n'

def call_deepseek_api_stream(user_prompt, log_callback=None):
    """
    è°ƒç”¨DeepSeek R1 APIè¿›è¡Œæ¢æµ‹å™¨è®¾è®¡ï¼ˆæµå¼è¾“å‡ºï¼Œå¸¦æ¨ç†è¿‡ç¨‹ï¼‰
    
    Args:
        user_prompt: ç”¨æˆ·çš„è®¾è®¡éœ€æ±‚
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°ï¼Œç”¨äºå®æ—¶è¾“å‡ºAIæ€è€ƒè¿‡ç¨‹
        
    Returns:
        dict: åŒ…å«statusã€contentå’Œreasoning_contentçš„å­—å…¸
    """
    if not DEEPSEEK_API_KEY:
        return {
            'status': 'error',
            'message': 'APIå¯†é’¥æœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEY'
        }
    
    try:
        if log_callback:
            log_callback('ğŸ”— è¿æ¥æ¨ç†å¤§æ¨¡å‹ API...')
        
        messages = [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': user_prompt}
        ]
        
        if log_callback:
            log_callback('ğŸ“¡ å‘é€è®¾è®¡è¯·æ±‚åˆ°æ·±åº¦å­¦ä¹ æ¨¡å‹...')
        
        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=messages,
            stream=True
        )
        
        if log_callback:
            log_callback('âœ… è¿æ¥æˆåŠŸï¼ŒR1å¼€å§‹æ¨ç†...')
        
        reasoning_content = ""
        content = ""
        reasoning_count = 0
        content_count = 0
        
        for chunk in response:
            # å¤„ç†æ¨ç†è¿‡ç¨‹
            if chunk.choices[0].delta.reasoning_content:
                reasoning_chunk = chunk.choices[0].delta.reasoning_content
                reasoning_content += reasoning_chunk
                reasoning_count += 1
                
                # æ¯æ”¶åˆ°20ä¸ªæ¨ç†chunkè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if reasoning_count % 20 == 0 and log_callback:
                    log_callback(f' AIæ­£åœ¨æ·±åº¦æ¨ç†... (æ¨ç† {len(reasoning_content)} å­—ç¬¦)')
            
            # å¤„ç†æœ€ç»ˆå†…å®¹
            elif chunk.choices[0].delta.content:
                content_chunk = chunk.choices[0].delta.content
                content += content_chunk
                content_count += 1
                
                # æ¯æ”¶åˆ°10ä¸ªå†…å®¹chunkè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if content_count % 10 == 0 and log_callback:
                    log_callback(f' AIæ­£åœ¨ç”Ÿæˆæ–¹æ¡ˆ... (å·²ç”Ÿæˆ {len(content)} å­—ç¬¦)')
        
        if log_callback:
            log_callback(f' æ¨ç†å®Œæˆï¼æ¨ç†è¿‡ç¨‹ {len(reasoning_content)} å­—ç¬¦ï¼Œæ–¹æ¡ˆ {len(content)} å­—ç¬¦')
        
        return {
            'status': 'success',
            'content': content,
            'reasoning_content': reasoning_content
        }
        
    except Exception as e:
        if log_callback:
            log_callback(f'âŒ APIè°ƒç”¨å¤±è´¥: {str(e)}')
        return {
            'status': 'error',
            'message': f'APIè°ƒç”¨å¤±è´¥: {str(e)}'
        }
