import os
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

import json
import time
from utils import extract_json_from_text

# å¯¼å…¥ RAG æœåŠ¡
try:
    from rag_service import _retriever, initialize_rag
    RAG_AVAILABLE = True
    # ç¡®ä¿ RAG ç³»ç»Ÿå·²åˆå§‹åŒ–
    initialize_rag()
except Exception as e:
    print(f"RAG ç³»ç»Ÿä¸å¯ç”¨: {e}")
    RAG_AVAILABLE = False
    _retriever = None

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨DeepSeekçš„base_url
if DEEPSEEK_API_KEY:
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")
else:
    client = None

def get_academic_references(user_prompt):
    """
    ä»çŸ¥è¯†åº“æ£€ç´¢ä¸è®¾è®¡éœ€æ±‚ç›¸å…³çš„å­¦æœ¯æ–‡çŒ®
    
    Args:
        user_prompt: ç”¨æˆ·çš„è®¾è®¡éœ€æ±‚
        
    Returns:
        str: ç›¸å…³å­¦æœ¯æ–‡çŒ®å†…å®¹ï¼Œå¦‚æœ RAG ä¸å¯ç”¨åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not RAG_AVAILABLE or _retriever is None:
        return ""
    
    try:
        # æ„å»ºæ£€ç´¢æŸ¥è¯¢ï¼Œæå–å…³é”®ä¿¡æ¯
        search_queries = [
            f"å…‰ç”µæ¢æµ‹å™¨è®¾è®¡ {user_prompt}",
            "å å±‚å…‰ç”µæ¢æµ‹å™¨ç»“æ„",
            "é‡å­æ•ˆç‡ä¼˜åŒ–æ–¹æ³•"
        ]
        
        all_docs = []
        for query in search_queries:
            docs = _retriever.invoke(query)
            all_docs.extend(docs)
        
        # å»é‡å¹¶é™åˆ¶æ–‡æ¡£æ•°é‡
        unique_docs = []
        seen_content = set()
        for doc in all_docs:
            content_hash = hash(doc.page_content[:100])  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºå»é‡ä¾æ®
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_docs.append(doc)
                if len(unique_docs) >= 5:  # æœ€å¤š5ä¸ªæ–‡æ¡£ç‰‡æ®µ
                    break
        
        if not unique_docs:
            return ""
        
        # æ ¼å¼åŒ–æ–‡çŒ®å†…å®¹
        references = "\n\n".join([
            f"ã€å‚è€ƒæ–‡çŒ®ç‰‡æ®µ {i+1}ã€‘\n{doc.page_content}"
            for i, doc in enumerate(unique_docs)
        ])
        
        return references
        
    except Exception as e:
        print(f"æ£€ç´¢å­¦æœ¯æ–‡çŒ®å¤±è´¥: {e}")
        return ""

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å…‰ç”µæ¢æµ‹å™¨è®¾è®¡ä¸“å®¶ï¼Œç²¾é€šåŠå¯¼ä½“ç‰©ç†ã€ææ–™ç§‘å­¦å’Œå…‰ç”µå™¨ä»¶å·¥ç¨‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„å‚æ•°ï¼Œè®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„å å±‚å…‰ç”µæ¢æµ‹å™¨ã€‚

è®¾è®¡æ—¶éœ€è¦è€ƒè™‘ï¼š
1. ææ–™çš„ç¦å¸¦å®½åº¦ä¸ç›®æ ‡æ³¢é•¿çš„åŒ¹é…
2. å„å±‚åšåº¦å¯¹å…‰å¸æ”¶å’Œè½½æµå­æ”¶é›†çš„å½±å“
3. å¼‚è´¨ç»“ç•Œé¢çš„èƒ½å¸¦åŒ¹é…
4. æš—ç”µæµçš„æŠ‘åˆ¶
5. é‡å­æ•ˆç‡çš„ä¼˜åŒ–

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºè®¾è®¡ç»“æœï¼ŒåŒ…å«layersï¼ˆå±‚ç»“æ„ï¼‰ã€performanceï¼ˆæ€§èƒ½å‚æ•°ï¼‰ã€optimization_suggestionsï¼ˆä¼˜åŒ–å»ºè®®ï¼‰å’Œexplanationï¼ˆè®¾è®¡è¯´æ˜ï¼‰ã€‚
"""

SYSTEM_PROMPT_WITH_RAG = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å…‰ç”µæ¢æµ‹å™¨è®¾è®¡ä¸“å®¶ï¼Œç²¾é€šåŠå¯¼ä½“ç‰©ç†ã€ææ–™ç§‘å­¦å’Œå…‰ç”µå™¨ä»¶å·¥ç¨‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„å‚æ•°å’Œå­¦æœ¯æ–‡çŒ®èµ„æ–™ï¼Œè®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„å å±‚å…‰ç”µæ¢æµ‹å™¨ã€‚

**é‡è¦è¦æ±‚**ï¼š
1. ä½ å¿…é¡»å‚è€ƒæä¾›çš„ã€å­¦æœ¯æ–‡çŒ®ã€‘ä¸­çš„è®¾è®¡æ€è·¯ã€ææ–™é€‰æ‹©å’Œæ€§èƒ½å‚æ•°
2. è®¾è®¡æ–¹æ¡ˆåº”åŸºäºæ–‡çŒ®ä¸­çš„å®éªŒæ•°æ®å’Œç†è®ºåˆ†æ
3. åœ¨ explanation å­—æ®µä¸­æ˜ç¡®è¯´æ˜ä½ å‚è€ƒäº†å“ªäº›æ–‡çŒ®å†…å®¹ï¼Œä»¥åŠå¦‚ä½•åº”ç”¨è¿™äº›çŸ¥è¯†
4. ä¼˜åŒ–å»ºè®®åº”ç»“åˆæ–‡çŒ®ä¸­çš„æœ€æ–°ç ”ç©¶æˆæœ

è®¾è®¡æ—¶éœ€è¦è€ƒè™‘ï¼š
1. ææ–™çš„ç¦å¸¦å®½åº¦ä¸ç›®æ ‡æ³¢é•¿çš„åŒ¹é…ï¼ˆå‚è€ƒæ–‡çŒ®ä¸­çš„ææ–™ç‰¹æ€§ï¼‰
2. å„å±‚åšåº¦å¯¹å…‰å¸æ”¶å’Œè½½æµå­æ”¶é›†çš„å½±å“ï¼ˆå‚è€ƒæ–‡çŒ®ä¸­çš„ä¼˜åŒ–å‚æ•°ï¼‰
3. å¼‚è´¨ç»“ç•Œé¢çš„èƒ½å¸¦åŒ¹é…ï¼ˆå‚è€ƒæ–‡çŒ®ä¸­çš„ç•Œé¢å·¥ç¨‹ï¼‰
4. æš—ç”µæµçš„æŠ‘åˆ¶ï¼ˆå‚è€ƒæ–‡çŒ®ä¸­çš„æŠ‘åˆ¶ç­–ç•¥ï¼‰
5. é‡å­æ•ˆç‡çš„ä¼˜åŒ–ï¼ˆå‚è€ƒæ–‡çŒ®ä¸­çš„æ•ˆç‡æå‡æ–¹æ³•ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºè®¾è®¡ç»“æœï¼ŒåŒ…å«layersï¼ˆå±‚ç»“æ„ï¼‰ã€performanceï¼ˆæ€§èƒ½å‚æ•°ï¼‰ã€optimization_suggestionsï¼ˆä¼˜åŒ–å»ºè®®ï¼‰å’Œexplanationï¼ˆè®¾è®¡è¯´æ˜ï¼Œå¿…é¡»å¼•ç”¨æ–‡çŒ®ï¼‰ã€‚
"""

def call_deepseek_api(user_prompt, model="deepseek-reasoner", use_rag=True):
    """
    è°ƒç”¨DeepSeek APIè¿›è¡Œæ¢æµ‹å™¨è®¾è®¡ï¼ˆéæµå¼ï¼‰
    
    Args:
        user_prompt: ç”¨æˆ·æç¤ºè¯
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "deepseek-reasoner" (R1)ï¼Œå¯é€‰ "deepseek-chat" (V3)
        use_rag: æ˜¯å¦ä½¿ç”¨ RAG å¢å¼ºï¼Œé»˜è®¤ä¸º True
    """
    if not client:
        return {
            'status': 'error',
            'message': 'APIå¯†é’¥æœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEY'
        }
    
    try:
        # æ£€ç´¢å­¦æœ¯æ–‡çŒ®
        academic_refs = ""
        if use_rag and RAG_AVAILABLE:
            academic_refs = get_academic_references(user_prompt)
        
        # æ ¹æ®æ˜¯å¦æœ‰æ–‡çŒ®é€‰æ‹©ä¸åŒçš„ç³»ç»Ÿæç¤ºè¯
        if academic_refs:
            system_prompt = SYSTEM_PROMPT_WITH_RAG
            # å°†æ–‡çŒ®æ·»åŠ åˆ°ç”¨æˆ·æç¤ºè¯ä¸­
            enhanced_prompt = f"""ã€å­¦æœ¯æ–‡çŒ®ã€‘
{academic_refs}

ã€è®¾è®¡éœ€æ±‚ã€‘
{user_prompt}

è¯·åŸºäºä»¥ä¸Šå­¦æœ¯æ–‡çŒ®å’Œè®¾è®¡éœ€æ±‚ï¼Œç»™å‡ºä¸“ä¸šçš„æ¢æµ‹å™¨è®¾è®¡æ–¹æ¡ˆã€‚"""
        else:
            system_prompt = SYSTEM_PROMPT
            enhanced_prompt = user_prompt
        
        messages = [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': enhanced_prompt}
        ]
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            stream=False
        )
        
        content = response.choices[0].message.content
        
        return {
            'status': 'success',
            'content': content,
            'used_rag': bool(academic_refs)
        }
        
    except Exception as e:
        return {
            'status': 'error',
            'message': f'APIè°ƒç”¨å¤±è´¥: {str(e)}'
        }

def generate_design_stream(prompt, model_type='deepseek-reasoner', use_rag=True):
    """
    ç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæµå¼è°ƒç”¨DeepSeek APIå¹¶è¿”å›ç‰¹å®šæ ¼å¼çš„è¿›åº¦æ•°æ®
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        model_type: æ¨¡å‹ç±»å‹
        use_rag: æ˜¯å¦ä½¿ç”¨ RAG å¢å¼ºï¼Œé»˜è®¤ä¸º True
    """
    if not client:
        yield json.dumps({
            'step': -1,
            'error': 'APIå¯†é’¥æœªé…ç½®',
            'progress': 0
        }) + '\n'
        return

    # ç¡®å®šæ¨¡å‹æ˜¾ç¤ºåç§°
    model_display = "AIæ¨ç†æ¨¡å‹" if model_type == 'deepseek-reasoner' else "AIå¤§æ¨¡å‹"

    # RAG æ£€ç´¢é˜¶æ®µ
    academic_refs = ""
    if use_rag and RAG_AVAILABLE:
        yield json.dumps({'step': 3, 'message': 'ğŸ“š æ£€ç´¢å­¦æœ¯æ–‡çŒ®...', 'progress': 25, 'log': True}) + '\n'
        time.sleep(0.2)
        
        academic_refs = get_academic_references(prompt)
        
        if academic_refs:
            yield json.dumps({'step': 3, 'message': 'âœ… å·²æ£€ç´¢åˆ°ç›¸å…³å­¦æœ¯æ–‡çŒ®ï¼Œå°†ç”¨äºå¢å¼ºè®¾è®¡', 'progress': 28, 'log': True}) + '\n'
        else:
            yield json.dumps({'step': 3, 'message': 'âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ï¼Œä½¿ç”¨æ ‡å‡†è®¾è®¡æ¨¡å¼', 'progress': 28, 'log': True}) + '\n'
        time.sleep(0.2)

    yield json.dumps({'step': 3, 'message': f'è°ƒç”¨{model_display} API', 'progress': 30}) + '\n'
    time.sleep(0.3)
    
    yield json.dumps({'step': 3, 'message': f'ğŸ”— è¿æ¥{model_display} API...', 'progress': 35, 'log': True}) + '\n'
    yield json.dumps({'step': 3, 'message': f'ğŸ“¡ å‘é€è®¾è®¡è¯·æ±‚åˆ°{model_display}æ¨¡å‹...', 'progress': 38, 'log': True}) + '\n'
    yield json.dumps({'step': 3, 'message': f'âœ… è¿æ¥æˆåŠŸï¼Œ{model_display}å¼€å§‹æ¨ç†...', 'progress': 40, 'log': True}) + '\n'

    try:
        # æ ¹æ®æ˜¯å¦æœ‰æ–‡çŒ®é€‰æ‹©ä¸åŒçš„ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯
        if academic_refs:
            system_prompt = SYSTEM_PROMPT_WITH_RAG
            enhanced_prompt = f"""ã€å­¦æœ¯æ–‡çŒ®ã€‘
{academic_refs}

ã€è®¾è®¡éœ€æ±‚ã€‘
{prompt}

è¯·åŸºäºä»¥ä¸Šå­¦æœ¯æ–‡çŒ®å’Œè®¾è®¡éœ€æ±‚ï¼Œç»™å‡ºä¸“ä¸šçš„æ¢æµ‹å™¨è®¾è®¡æ–¹æ¡ˆã€‚åœ¨è®¾è®¡è¯´æ˜ä¸­è¯·æ˜ç¡®å¼•ç”¨æ–‡çŒ®å†…å®¹ã€‚"""
        else:
            system_prompt = SYSTEM_PROMPT
            enhanced_prompt = prompt
        
        messages = [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': enhanced_prompt}
        ]
        
        response = client.chat.completions.create(
            model=model_type,
            messages=messages,
            stream=True
        )
        
        reasoning_content = ""
        content = ""
        reasoning_count = 0
        content_count = 0
        
        for chunk in response:
            # å¤„ç†æ¨ç†è¿‡ç¨‹ï¼ˆä»…R1æ¨¡å‹æœ‰ï¼‰
            if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                reasoning_chunk = chunk.choices[0].delta.reasoning_content
                reasoning_content += reasoning_chunk
                reasoning_count += 1
                
                # æ¯æ”¶åˆ°5ä¸ªæ¨ç†chunkè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if reasoning_count % 5 == 0:
                    current_progress = min(40 + reasoning_count // 5, 60)
                    preview = reasoning_content[-50:].replace('\n', ' ')
                    yield json.dumps({
                        'step': 4,
                        'message': f' {model_display}æ¨ç†ä¸­: ...{preview}',
                        'progress': current_progress,
                        'log': True
                    }) + '\n'
            
            # å¤„ç†æœ€ç»ˆå†…å®¹
            elif chunk.choices[0].delta.content:
                content_chunk = chunk.choices[0].delta.content
                content += content_chunk
                content_count += 1
                
                # æ¯æ”¶åˆ°3ä¸ªå†…å®¹chunkè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if content_count % 3 == 0:
                    current_progress = min(60 + content_count // 3, 70)
                    yield json.dumps({
                        'step': 4,
                        'message': f'âœ¨ {model_display}ç”Ÿæˆæ–¹æ¡ˆä¸­... (å·²ç”Ÿæˆ {len(content)} å­—ç¬¦)',
                        'progress': current_progress,
                        'log': True
                    }) + '\n'
        
        yield json.dumps({
            'step': 4,
            'message': f' æ¨ç†å®Œæˆï¼æ¨ç† {len(reasoning_content)} å­—ç¬¦ï¼Œæ–¹æ¡ˆ {len(content)} å­—ç¬¦',
            'progress': 75,
            'log': True
        }) + '\n'
        
        # è¿”å›æœ€ç»ˆçš„å®Œæ•´å†…å®¹ï¼Œä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„æ¶ˆæ¯ç±»å‹ï¼Œæˆ–è€…è®©è°ƒç”¨è€…è‡ªå·±è§£æ
        # è¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥yieldç»“æœå¯¹è±¡ï¼Œè€Œæ˜¯è®©è°ƒç”¨è€…çŸ¥é“APIè°ƒç”¨å·²å®Œæˆï¼Œå¹¶æä¾›å†…å®¹
        # ä½†ä¸ºäº†ä¿æŒæµçš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç”Ÿæˆå™¨æœ€åè¿”å›ç»“æœ
        
        yield json.dumps({'step': 5, 'message': 'è§£æè®¾è®¡æ–¹æ¡ˆ', 'progress': 80}) + '\n'
        time.sleep(0.5)

        # ä½¿ç”¨ utils ä¸­çš„å‡½æ•°è§£æ
        design_data = extract_json_from_text(content)
        
        if not design_data:
            yield json.dumps({
                'step': -1,
                'error': 'APIè¿”å›çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œæ— æ³•è§£æä¸ºJSON',
                'progress': 0
            }) + '\n'
            return

        # æˆåŠŸè§£æï¼Œå°†æ•°æ®ä¼ å›
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”¨ä¸€ä¸ªç‰¹æ®Šçš„ type æ¥æ ‡è¯†è¿™æ˜¯æœ€ç»ˆæ•°æ®
        yield json.dumps({
            'type': 'result',
            'design_data': design_data,
            'reasoning_content': reasoning_content
        }) + '\n'

    except Exception as e:
        yield json.dumps({
            'step': -1,
            'error': f'APIè°ƒç”¨å¤±è´¥: {str(e)}',
            'progress': 0
        }) + '\n'

def call_deepseek_api_stream(user_prompt, log_callback=None):
    """
    è°ƒç”¨DeepSeek R1 APIè¿›è¡Œæ¢æµ‹å™¨è®¾è®¡ï¼ˆæµå¼è¾“å‡ºï¼Œå¸¦æ¨ç†è¿‡ç¨‹ï¼‰
    
    Args:
        user_prompt: ç”¨æˆ·çš„è®¾è®¡éœ€æ±‚
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°ï¼Œç”¨äºå®æ—¶è¾“å‡ºAIæ€è€ƒè¿‡ç¨‹
        
    Returns:
        dict: åŒ…å«statusã€contentå’Œreasoning_contentçš„å­—å…¸
    """
    if not DEEPSEEK_API_KEY:
        return {
            'status': 'error',
            'message': 'APIå¯†é’¥æœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEY'
        }
    
    try:
        if log_callback:
            log_callback('ğŸ”— è¿æ¥æ¨ç†å¤§æ¨¡å‹ API...')
        
        messages = [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': user_prompt}
        ]
        
        if log_callback:
            log_callback('ğŸ“¡ å‘é€è®¾è®¡è¯·æ±‚åˆ°æ·±åº¦å­¦ä¹ æ¨¡å‹...')
        
        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=messages,
            stream=True
        )
        
        if log_callback:
            log_callback('âœ… è¿æ¥æˆåŠŸï¼ŒR1å¼€å§‹æ¨ç†...')
        
        reasoning_content = ""
        content = ""
        reasoning_count = 0
        content_count = 0
        
        for chunk in response:
            # å¤„ç†æ¨ç†è¿‡ç¨‹
            if chunk.choices[0].delta.reasoning_content:
                reasoning_chunk = chunk.choices[0].delta.reasoning_content
                reasoning_content += reasoning_chunk
                reasoning_count += 1
                
                # æ¯æ”¶åˆ°20ä¸ªæ¨ç†chunkè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if reasoning_count % 20 == 0 and log_callback:
                    log_callback(f' AIæ­£åœ¨æ·±åº¦æ¨ç†... (æ¨ç† {len(reasoning_content)} å­—ç¬¦)')
            
            # å¤„ç†æœ€ç»ˆå†…å®¹
            elif chunk.choices[0].delta.content:
                content_chunk = chunk.choices[0].delta.content
                content += content_chunk
                content_count += 1
                
                # æ¯æ”¶åˆ°10ä¸ªå†…å®¹chunkè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if content_count % 10 == 0 and log_callback:
                    log_callback(f' AIæ­£åœ¨ç”Ÿæˆæ–¹æ¡ˆ... (å·²ç”Ÿæˆ {len(content)} å­—ç¬¦)')
        
        if log_callback:
            log_callback(f' æ¨ç†å®Œæˆï¼æ¨ç†è¿‡ç¨‹ {len(reasoning_content)} å­—ç¬¦ï¼Œæ–¹æ¡ˆ {len(content)} å­—ç¬¦')
        
        return {
            'status': 'success',
            'content': content,
            'reasoning_content': reasoning_content
        }
        
    except Exception as e:
        if log_callback:
            log_callback(f'âŒ APIè°ƒç”¨å¤±è´¥: {str(e)}')
        return {
            'status': 'error',
            'message': f'APIè°ƒç”¨å¤±è´¥: {str(e)}'
        }
